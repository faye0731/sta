{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7389faa1",
   "metadata": {
    "id": "7389faa1"
   },
   "source": [
    "# Homework 5: Part I\n",
    "\n",
    "1. Go get data from kaggle.com and do a ***Bayesian Linear Regression*** analysis\n",
    "\n",
    "```python\n",
    "import pymc as pm; import numpy as np\n",
    "n,p=100,10; X,y=np.zeros((n,p)),np.ones((n,1))\n",
    "# Replace this made up data with your data set from kaggle...\n",
    "with pm.Model() as MLR:\n",
    "    betas = pm.MvNormal('betas', mu=np.zeros((p,1)), cov=np.eye(p), shape=(p,1))\n",
    "    sigma = pm.TruncatedNormal('sigma', mu=1, sigma=1, lower=0) # half normal\n",
    "    y = pm.Normal('y', mu=pm.math.dot(X, betas), sigma=sigma, observed=y)\n",
    "\n",
    "with MLR:\n",
    "    idata = pm.sample()\n",
    "```    \n",
    "\n",
    "2. Choose ***prior*** that are sensible: certainly you might change the ***hyperparameters***, and perhaps you can experiment with different distributional families for `sigma`...\n",
    "\n",
    "3. [Optional] Assess the performance of the MCMC and note any issues or warnings\n",
    "\n",
    "    1. Traceplots, inference (credible) intervals, effective sample sizes, energy plots, warnings and other notes... just the usual stuff they do [here](https://www.pymc.io/projects/docs/en/stable/learn/core_notebooks/pymc_overview.html#pymc-overview)\n",
    "\n",
    "4. [Optional] Perform ***Multiple Linear Regression*** diagnostics... residual plots, etc.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "q9NmYEDiQ3HW",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 57
    },
    "id": "q9NmYEDiQ3HW",
    "outputId": "8bd77064-53d9-4532-f75e-06616598cea1"
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'pd' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpymc\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mpm\u001b[39;00m; \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnp\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[43mpd\u001b[49m\u001b[38;5;241m.\u001b[39mread_csv(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtoronto-movies.csv\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m      3\u001b[0m X \u001b[38;5;241m=\u001b[39m data[[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmetascore\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mrated\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mgenre\u001b[39m\u001b[38;5;124m'\u001b[39m]]\u001b[38;5;241m.\u001b[39mvalues\n\u001b[1;32m      4\u001b[0m y \u001b[38;5;241m=\u001b[39m data[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mimdb_rating\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mvalues\n",
      "\u001b[0;31mNameError\u001b[0m: name 'pd' is not defined"
     ]
    }
   ],
   "source": [
    "import pymc as pm\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "data = pd.read_csv('toronto-movies.csv')\n",
    "X = data[['metascore', 'rated', 'genre']].values\n",
    "y = data['imdb_rating'].values\n",
    "n,p=100,10; X,y=np.zeros((n,p)),np.ones((n,1))\n",
    "\n",
    "with pm.Model() as MLR:\n",
    "    betas = pm.MvNormal('betas', mu=np.zeros((p,1)), cov=np.eye(p), shape=(p,1))\n",
    "    sigma = pm.TruncatedNormal('sigma', mu=1, sigma=1, lower=0) # half normal\n",
    "    y = pm.Normal('y', mu=pm.math.dot(X, betas), sigma=sigma, observed=y)\n",
    "\n",
    "with MLR:\n",
    "    idata = pm.sample()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba9d8081",
   "metadata": {
    "id": "ba9d8081"
   },
   "source": [
    "# Homework 5: Part II\n",
    "    \n",
    "## Answer the following with respect to $p(\\boldsymbol \\beta |\\boldsymbol\\Sigma, \\mathbf{X},\\mathbf{y})$ on the previous slide\n",
    "    \n",
    "1. Rewrite $p(\\boldsymbol \\beta |\\boldsymbol\\Sigma, \\mathbf{X},\\mathbf{y})$ in terms of $\\sigma^2$ (no longer using $\\Sigma$) if $\\Sigma=\\sigma^2I$\n",
    "\n",
    "2. What is $E[\\boldsymbol \\beta |\\boldsymbol\\Sigma, \\mathbf{X},\\mathbf{y}]$?\n",
    "\n",
    "3. What ***hyperparameters*** values (legal or illegal) would make $E[\\boldsymbol \\beta |\\boldsymbol\\Sigma, \\mathbf{X},\\mathbf{y}] = (\\mathbf{X^\\top X})^{-1}\\mathbf{X^\\top y}$?\n",
    "\n",
    "4. What ***hyperparameters*** values (legal or illegal) would make $E[  \\mathbf{\\hat y} = \\mathbf{X}\\boldsymbol \\beta |\\boldsymbol\\Sigma, \\mathbf{X},\\mathbf{y}] = \\mathbf{X}(\\mathbf{X^\\top X})^{-1}\\mathbf{X^\\top y}$?\n",
    "\n",
    "5. What is $\\text{Var}[\\boldsymbol \\beta |\\boldsymbol\\Sigma, \\mathbf{X},\\mathbf{y}]$?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd3dbf08",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "# Homework 5: Part II Answered in Comment Format for Python\n",
    "\n",
    "# 1. Rewrite p(beta | Sigma, X, y) in terms of sigma^2 (no longer using Sigma) if Sigma=sigma^2I\n",
    "Given that Sigma = sigma^2I, the expression for the posterior distribution\n",
    "p(beta | Sigma, X, y) can be rewritten to incorporate sigma^2 directly.\n",
    "This is because Sigma being equal to sigma^2I simplifies the covariance matrix to a scalar\n",
    "multiplied by the identity matrix, indicating that the errors are homoscedastic and\n",
    "independently distributed with variance sigma^2.\n",
    "\n",
    "\n",
    "# 2. What is E[beta | Sigma, X, y]?\n",
    "The expected value of beta given Sigma, X, and y, denoted as E[beta | Sigma, X, y],\n",
    "is the mean of the posterior distribution of the beta coefficients. In the context of Bayesian\n",
    "linear regression, this would typically be derived from the posterior distribution using Bayes' theorem,\n",
    "factoring in the likelihood of the data given the parameters and the prior distribution of the parameters.\n",
    "\n",
    "\n",
    "# 3. What hyperparameters values (legal or illegal) would make E[beta | Sigma, X, y] = (X^T X)^{-1}X^T y?\n",
    "To make the expected value of beta equal to (X^T X)^{-1}X^T y, the prior distribution of beta must be\n",
    "set such that it reflects a non-informative or flat prior. This effectively reduces the Bayesian model\n",
    "to the ordinary least squares (OLS) estimator. Therefore, the hyperparameters that would result in this\n",
    "are those that minimize the influence of the prior on the posterior, essentially making the prior\n",
    "distribution of beta uniform or assigning a very large variance to it.\n",
    "\n",
    "\n",
    "# 4. What hyperparameters values (legal or illegal) would make E[ y_hat = X beta | Sigma, X, y] = X(X^T X)^{-1}X^T y?\n",
    "This question essentially asks for the conditions under which the expected value of the predicted y\n",
    "(y_hat) equals the OLS prediction. This occurs when the beta coefficients are estimated as in OLS,\n",
    "which, as mentioned in answer 3, requires the prior on beta to be non-informative or to have a very\n",
    "large variance. Therefore, the same hyperparameter conditions apply here.\n",
    "\n",
    "\n",
    "# 5. What is Var[beta | Sigma, X, y]?\n",
    "The variance of beta given Sigma, X, and y, denoted as Var[beta | Sigma, X, y], represents the uncertainty\n",
    "or spread of the beta coefficients in their posterior distribution. In the context of a Bayesian linear\n",
    "regression model with Sigma = sigma^2I, the variance of beta can often be expressed as sigma^2(X^T X)^{-1},\n",
    "which aligns with the covariance matrix of the OLS estimator but now interpreted in a Bayesian framework.\n",
    "This variance depends on the inverse of the design matrix (X^T X) and the variance of the errors (sigma^2),\n",
    "reflecting how data informativeness and error variability influence uncertainty in the coefficient estimates.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7ad2ed1",
   "metadata": {
    "id": "b7ad2ed1"
   },
   "source": [
    "# Homework 5: Part III\n",
    "\n",
    "1. Go get data from kaggle.com and perform inference for a ***Bayesian Multivariate Normal Model***\n",
    "\n",
    "<SPAN STYLE=\"font-size:18.0pt\">\n",
    "\n",
    "```python\n",
    "import numpy as np; from scipy import stats\n",
    "p=10; Psi=np.eye(p); a_cov = stats.invwishart(df=p+2, scale=Psi).rvs(1)\n",
    "n=1000; y=stats.multivariate_normal(mean=np.zeros(p), cov=a_cov).rvs(size=n)\n",
    "# Replace this made up data with your data set from kaggle...\n",
    "    \n",
    "with pm.Model() as MNV_LKJ:\n",
    "    packed_L = pm.LKJCholeskyCov(\"packed_L\", n=p, eta=2.0,\n",
    "                                 sd_dist=pm.Exponential.dist(1.0, shape=2), compute_corr=False)\n",
    "    L = pm.expand_packed_triangular(p, packed_L)\n",
    "    # Sigma = pm.Deterministic('Sigma', L.dot(L.T)) # Don't use a covariance matrix parameterization\n",
    "    mu = pm.MvNormal('mu', mu=np.array(0), cov=np.eye(p), shape=p);\n",
    "    # y = pm.MvNormal('y', mu=mu, cov=Sigma, shape=(n,1), observed=y)\n",
    "    # Figure out how to parameterize this with a Cholesky factor to improve computational efficiency\n",
    "with MNV_LKJ    \n",
    "    idata = pm.sample()\n",
    "```    \n",
    "</SPAN>\n",
    "\n",
    "2. As indicated above, don't use a covariance matrix parameterization and instead figure out how to parameterize this with a ***Cholesky factor*** to improve computational efficiency. The ***Cholesky***-based formulation allows general $O(n^3)$ $\\det({\\boldsymbol \\Sigma})$ to be computed using a simple $O(n)$ product and general $O(n^3)$ ${\\boldsymbol \\Sigma}^{-1}$ to be instead evaluated with $O(n^2)$ ***backward substitution***.\n",
    "\n",
    "2. Specify ***priors*** that work: certainly you'll likely need to change the ***prior hyperparameters*** for $\\boldsymbol \\mu$ (`mu`) and $\\mathbf{R}$ (`packed_L`)...\n",
    "    1. And you could consider adjusting the ***prior*** for $\\boldsymbol \\sigma$ using `sd_dist`...\n",
    "\n",
    "3. [Optional] Assess the performance of the MCMC and note any issues\n",
    "\n",
    "    1. Traceplots, inference (credible) intervals, effective sample sizes, energy plots, warnings and other notes... just the usual stuff they do [here](https://www.pymc.io/projects/docs/en/stable/learn/core_notebooks/pymc_overview.html#pymc-overview)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "Kyo3ww8OYydi",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 57
    },
    "id": "Kyo3ww8OYydi",
    "outputId": "e583303e-e41f-467c-f8f4-7ca4c93bb1b8"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "<style>\n",
       "    /* Turns off some styling */\n",
       "    progress {\n",
       "        /* gets rid of default border in Firefox and Opera. */\n",
       "        border: none;\n",
       "        /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "        background-size: auto;\n",
       "    }\n",
       "    progress:not([value]), progress:not([value])::-webkit-progress-bar {\n",
       "        background: repeating-linear-gradient(45deg, #7e7e7e, #7e7e7e 10px, #5c5c5c 10px, #5c5c5c 20px);\n",
       "    }\n",
       "    .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
       "        background: #F44336;\n",
       "    }\n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      <progress value='2000' class='' max='2000' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      100.00% [2000/2000 01:02&lt;00:00 Sampling chain 0, 0 divergences]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "<style>\n",
       "    /* Turns off some styling */\n",
       "    progress {\n",
       "        /* gets rid of default border in Firefox and Opera. */\n",
       "        border: none;\n",
       "        /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "        background-size: auto;\n",
       "    }\n",
       "    progress:not([value]), progress:not([value])::-webkit-progress-bar {\n",
       "        background: repeating-linear-gradient(45deg, #7e7e7e, #7e7e7e 10px, #5c5c5c 10px, #5c5c5c 20px);\n",
       "    }\n",
       "    .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
       "        background: #F44336;\n",
       "    }\n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      <progress value='2000' class='' max='2000' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      100.00% [2000/2000 00:55&lt;00:00 Sampling chain 1, 0 divergences]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import pymc as pm\n",
    "import numpy as np\n",
    "from scipy import stats\n",
    "\n",
    "p = 10  \n",
    "Psi = np.eye(p)  \n",
    "a_cov = stats.invwishart(df=p+5, scale=Psi).rvs(1)  \n",
    "n = 1000  \n",
    "y = stats.multivariate_normal(mean=np.zeros(p), cov=a_cov).rvs(size=n)  \n",
    "\n",
    "with pm.Model() as MNV_LKJ:\n",
    "    packed_L = pm.LKJCholeskyCov(\"packed_L\", n=p, eta=1.0,  \n",
    "                                 sd_dist=pm.Exponential.dist(1.0, shape=p), compute_corr=False)\n",
    "    L = pm.expand_packed_triangular(p, packed_L)\n",
    "    Sigma = pm.Deterministic('Sigma', L.dot(L.T)) \n",
    "    mu = pm.MvNormal('mu', mu=np.zeros(p), cov=np.eye(p)*1e-5, shape=p)  \n",
    "    y_obs = pm.MvNormal('y_obs', mu=mu, chol=L, observed=y)\n",
    "\n",
    "with MNV_LKJ:\n",
    "    trace = pm.sample(500)  "
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
